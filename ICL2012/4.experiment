Experiment

In order to gauge the efficacy of educate.me, a series of experiments are proposed. 

Does it scaffold learning effectively?

Is there a measurable difference in learning between independent research on an unknown topic scaffolded with educate.me versus completely free-form research? 
Extraneous variables, such as maturity of research methodology and technique, should be accounted for. Hence, a 2x2 factorial design[] is proposed.

Two groups--high school students and college graduates-- with two approaches --independent or with educate.me-- will research a candidate topic accessible to both groups--such as the chemical reactions at the core of the sun. A pre-post testing format will be used to determine

-- (a) Is there a statistically significant difference in retention between educate.me and independent research ?
-- (b) Is there a significant difference in the amount of time taken? (with educate.me tracking the time spent by each subject, and the independent research taking place in controlled condition.)

It is expected that users will take less time to do research and score higher using educate.me.

Is there a measurable difference in the sophistication of material desired by user after scaffolding with educate.me?

Does the sequenced progression of material from surface level popular treatment to expert-level detail actually r
esult in deep learning?

Since cognitive science suggests that cultivation of expertise requires about 10,000 hours[], it may be unfeasible for an experiment to try to conduct a longitudinal analysis long enough to measure this type of evolution. 

Instead, a protracted evaluation throughout the course of a semester is suggested for topics related to a course on engineering. Students will be asked to use educate.me for roughly 3 hours per week over the course of a semester to read articles on topics related to course material. After the semester, students will be asked to independently select subsequent readings on the content area. 
It is expected that students will pick pieces containing more in-depth coverage of material, such as a Communications of the ACM article on artificial intelligence versus an overview piece on Wired Magazine.

Are there other metrics that capture expertise, relevance, and the knowledge content better than the chosen model? 

What metrics best describe the content depth of piece of media, and how can we relate an individual's understanding to it?

For example, a socio-constructivist theory[] grounded metric may analyze the progression of a group of students in a cohort, keeping them in sync. 
This may result in the development of higher individual expertise as a result of communal interaction. 
Subsequent tests with different metrics will seek to identify the most effective set of parameters by replicating the learning difference experiments.